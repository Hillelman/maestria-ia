{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBbWMn9ebmi8"
      },
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## **Curso: Inteligencia Artificial y Aprendizaje Automático**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Actividad de las Semanas 5 y 6**\n",
        "### **Problema de asignación de créditos: South German Dataset.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiwEw8XsZG2W"
      },
      "source": [
        "## **Nombres y matrículas:**\n",
        "\n",
        "\n",
        "* ### ALICIA YOVANNA CANTA PANDAL - A01796095\n",
        "\n",
        "* ### HIRAM GARCIA AUSTRIA - A00378771\n",
        "\n",
        "* ### ALFREDO RIGOBERTO ALVAREZ SUAREZ - A01796142\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUxxkjE6shkH"
      },
      "source": [
        "# **Parte I: Partición, análisis y pre-procesamiento de los datos.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a08Di3GjkL1Y"
      },
      "source": [
        "## **Ejercicio 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "6VGEpE4SblfD"
      },
      "outputs": [],
      "source": [
        "# Aquí deberás incluir todas las librerías que requieras durante esta actividad:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate, GridSearchCV\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from imblearn.over_sampling import KMeansSMOTE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Bosque aleatorio\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "K3gUzoM3iRkt"
      },
      "outputs": [],
      "source": [
        "# Si se desean comentar algunos de los Warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "ppyf8TTgb9zq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>status</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration</th>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>credit_history</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>purpose</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amount</th>\n",
              "      <td>1049</td>\n",
              "      <td>2799</td>\n",
              "      <td>841</td>\n",
              "      <td>2122</td>\n",
              "      <td>2171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>savings</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>employment_duration</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>installment_rate</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>personal_status_sex</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other_debtors</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>present_residence</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>property</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>21</td>\n",
              "      <td>36</td>\n",
              "      <td>23</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other_installment_plans</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>housing</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_credits</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>job</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>people_liable</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>telephone</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>foreign_worker</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>credit_risk</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            0     1    2     3     4\n",
              "status                      1     1    2     1     1\n",
              "duration                   18     9   12    12    12\n",
              "credit_history              4     4    2     4     4\n",
              "purpose                     2     0    9     0     0\n",
              "amount                   1049  2799  841  2122  2171\n",
              "savings                     1     1    2     1     1\n",
              "employment_duration         2     3    4     3     3\n",
              "installment_rate            4     2    2     3     4\n",
              "personal_status_sex         2     3    2     3     3\n",
              "other_debtors               1     1    1     1     1\n",
              "present_residence           4     2    4     2     4\n",
              "property                    2     1    1     1     2\n",
              "age                        21    36   23    39    38\n",
              "other_installment_plans     3     3    3     3     1\n",
              "housing                     1     1    1     1     2\n",
              "number_credits              1     2    1     2     2\n",
              "job                         3     3    2     2     2\n",
              "people_liable               2     1    2     1     2\n",
              "telephone                   1     1    1     1     1\n",
              "foreign_worker              2     2    2     1     1\n",
              "credit_risk                 1     1    1     1     1"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Carga y renombra los nombres de las columnas del alemán al inglés y desplegamos\n",
        "# de nuevo el DataFrame para ver el resultado obtenido:\n",
        "\n",
        "# ************* Incluye aquí tu código:*****************************\n",
        "\n",
        "df = pd.read_csv('./SouthGermanCredit.asc', sep=' ')\n",
        "df.columns = [\n",
        "    'status',\n",
        "    'duration',\n",
        "    'credit_history',\n",
        "    'purpose',\n",
        "    'amount',\n",
        "    'savings',\n",
        "    'employment_duration',\n",
        "    'installment_rate',\n",
        "    'personal_status_sex',\n",
        "    'other_debtors',\n",
        "    'present_residence',\n",
        "    'property',\n",
        "    'age',\n",
        "    'other_installment_plans',\n",
        "    'housing',\n",
        "    'number_credits',\n",
        "    'job',\n",
        "    'people_liable',\n",
        "    'telephone',\n",
        "    'foreign_worker',\n",
        "    'credit_risk'\n",
        "]\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n",
        "\n",
        "df.head().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LckYCS8SlnFo"
      },
      "source": [
        "## **Ejercicio 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "Tlg8jYbnlqNA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "credit_risk\n",
            "1    700\n",
            "0    300\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Realiza a continuación una transformación para que la clase negativa (buen cliente)\n",
        "# quede con el valor de 0 y la clase positiva (mal cliente) quede con el valor de 1.\n",
        "\n",
        "# ************* Incluye aquí tu código:*****************************\n",
        "\n",
        "df['credit_risk'] = df['credit_risk'].replace({'1': 0, '0': 1})\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n",
        "\n",
        "\n",
        "print(df['credit_risk'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQx2lbzTkEsQ"
      },
      "source": [
        "## **Ejercicio 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "TP5-zdWVczhy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensiones:\n",
            "Entrenamiento: (700, 20) (700,)\n",
            "Prueba: (300, 20) (300,)\n",
            "\n",
            "Porcentaje clases Positiva:30.00%, y Negativa:70.00%\n"
          ]
        }
      ],
      "source": [
        "# Realiza una partición solicitada de entrenamiento y prueba.\n",
        "# Los nombres de los conjuntos deberán ser como se indican en los print de abajo:\n",
        "\n",
        "# ************* Incluye aquí tu código:*****************************\n",
        "\n",
        "X = df.drop('credit_risk', axis=1)\n",
        "y = df['credit_risk']\n",
        "\n",
        "# Mismos porcentajes que se utilizaron en el artículo: 70% - 30%\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n",
        "\n",
        "\n",
        "# Mostremos las dimensiones de la partición generada:\n",
        "print(\"Dimensiones:\")\n",
        "print(\"Entrenamiento:\", Xtrain.shape, ytrain.shape)\n",
        "print(\"Prueba:\", Xtest.shape, ytest.shape)\n",
        "\n",
        "# Y el porcentaje de cada clase de la variable de salida:\n",
        "tmp = ytrain.sum()/ytrain.shape[0]\n",
        "print(\"\\nPorcentaje clases Positiva:%.2f%%, y Negativa:%.2f%%\" % (100*(1-tmp),tmp*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptwAm_mOjVWH"
      },
      "source": [
        "### **Con base al porcentaje de los niveles de la variable de salida ¿podemos decir que tenemos un problema de datos desbalanceado? ¿Por qué?**\n",
        "\n",
        "\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "Tenemos un problema de clases desbalanceadas ya que la clase positiva (mal cliente) es minoritaria (30%), mientras que la clase negativa (buen cliente) es mayoritaria (70%).\n",
        "\n",
        "Para resolver este problema podemos realizar un balanceo de las clases en el conjunto de entrenamiento con técnicas de sobre-muestreo en la clase minoritaria, o bien técnicas de sub-muestreo en la clase mayoritaria.\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocmGcSC2j_2x"
      },
      "source": [
        "## **Ejercicio 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "5J2kIzLYHPZP"
      },
      "outputs": [],
      "source": [
        "# De acuerdo a la información de la Tabla 3 del artículo de la IEEE\n",
        "# define las variables correspondientes que se indican a continuación:\n",
        "\n",
        "# ************* Incluye aquí tu código:*****************************\n",
        "\n",
        "# Variables numéricas:\n",
        "lista_paper_num = [\n",
        "    'duration',\n",
        "    'amount',\n",
        "    'age',\n",
        "    'people_liable'\n",
        "]\n",
        "\n",
        "# Variables ordinales:\n",
        "lista_paper_ord = [\n",
        "    'employment_duration',\n",
        "    'installment_rate',\n",
        "    'present_residence',\n",
        "    'property',\n",
        "    'number_credits',\n",
        "    'job'\n",
        "]\n",
        "\n",
        "# Variables nominales:\n",
        "lista_paper_cat = [\n",
        "    'status',\n",
        "    'credit_history',\n",
        "    'purpose',\n",
        "    'savings',\n",
        "    'personal_status_sex',\n",
        "    'other_debtors',\n",
        "    'other_installment_plans',\n",
        "    'housing',\n",
        "    'telephone',\n",
        "    'foreign_worker',\n",
        "]\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb5bE4WJj8Rw"
      },
      "source": [
        "## **Ejercicio 5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "NvNYiIp9weCm"
      },
      "outputs": [],
      "source": [
        "# Transformaciones que se aplicarán a las variables numéricas usando la clase Pipeline de sklearn:\n",
        "\n",
        "# ************* Incluye aquí tu código:*****************************\n",
        "\n",
        "# Variables numéricas:\n",
        "numericas_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', MinMaxScaler()) # aplicando la misma normalización que el artículo\n",
        "])\n",
        "numericas_pipe_nombres = lista_paper_num\n",
        "\n",
        "\n",
        "# Variables categóricas-Nominales:\n",
        "nominales_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
        "])\n",
        "nominales_pipe_nombres = lista_paper_cat\n",
        "\n",
        "# Variables categóricas-ordinales:\n",
        "ordinales_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "ordinales_pipe_nombres = lista_paper_ord\n",
        "\n",
        "# Conjuntas las transformaciones de todo tipo de variable y\n",
        "# deja sin procesar aquellas que hayas decidido no transformar:\n",
        "\n",
        "columnasTransformer = ColumnTransformer([\n",
        "    ('numericas', numericas_pipe, numericas_pipe_nombres),\n",
        "    ('nominales', nominales_pipe, nominales_pipe_nombres),\n",
        "    ('ordinales', ordinales_pipe, ordinales_pipe_nombres)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJokj9Diyeu0"
      },
      "source": [
        "## **Ejercicio 6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "d3lqBiH5wd1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensión de las variables de entrada ANTES de las transformaciones: (1000, 20)\n",
            "Dimensión de las variables de entrada DESPUÉS de las transformaciones: (1000, 41)\n"
          ]
        }
      ],
      "source": [
        "# Como se va a utilizar Validación-Cruzada, concatena los conjuntos de entrenamiento y prueba\n",
        "# en uno nuevo conjunto aumentado que llamaremos trainval para utilizar como entrenamiento:\n",
        "\n",
        "\n",
        "# ************* Incluye aquí tu código:**************************\n",
        "\n",
        "Xtraintest = pd.concat([Xtrain, Xtest], axis=0)\n",
        "ytraintest = pd.concat([ytrain, ytest], axis=0)\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n",
        "\n",
        "\n",
        "# Veamos cuántas variables nuevas se introducen con las transformaciones One-Hot-Encoding:\n",
        "Xtmp = Xtraintest.copy()\n",
        "tmp = columnasTransformer.fit_transform(Xtmp)\n",
        "print(\"Dimensión de las variables de entrada ANTES de las transformaciones:\", Xtmp.shape)\n",
        "print(\"Dimensión de las variables de entrada DESPUÉS de las transformaciones:\", tmp.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxyRbHL0gNF"
      },
      "source": [
        "## **Ejercicio 7**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "0hdi7AAtwd5G",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> LR\n",
            "\t test_miaccuracy 0.815 (0.023)\n",
            "\t train_miaccuracy 0.831 (0.004)\n",
            "\t test_miprecision 0.807 (0.026)\n",
            "\t train_miprecision 0.821 (0.004)\n",
            "\t test_mirecall 0.827 (0.036)\n",
            "\t train_mirecall 0.846 (0.008)\n",
            "\t test_mifi 0.817 (0.024)\n",
            "\t train_mifi 0.833 (0.005)\n",
            "\t test_miauc 0.904 (0.022)\n",
            "\t train_miauc 0.922 (0.005)\n",
            "\t test_migmean 0.814 (0.023)\n",
            "\t train_migmean 0.831 (0.004)\n",
            ">> kNN\n",
            "\t test_miaccuracy 0.784 (0.027)\n",
            "\t train_miaccuracy 0.851 (0.008)\n",
            "\t test_miprecision 0.773 (0.033)\n",
            "\t train_miprecision 0.834 (0.013)\n",
            "\t test_mirecall 0.805 (0.038)\n",
            "\t train_mirecall 0.877 (0.008)\n",
            "\t test_mifi 0.788 (0.026)\n",
            "\t train_mifi 0.855 (0.008)\n",
            "\t test_miauc 0.861 (0.032)\n",
            "\t train_miauc 0.936 (0.006)\n",
            "\t test_migmean 0.783 (0.027)\n",
            "\t train_migmean 0.851 (0.009)\n",
            ">> DTree\n",
            "\t test_miaccuracy 0.773 (0.019)\n",
            "\t train_miaccuracy 1.000 (0.000)\n",
            "\t test_miprecision 0.779 (0.026)\n",
            "\t train_miprecision 1.000 (0.000)\n",
            "\t test_mirecall 0.763 (0.025)\n",
            "\t train_mirecall 1.000 (0.000)\n",
            "\t test_mifi 0.770 (0.019)\n",
            "\t train_mifi 1.000 (0.000)\n",
            "\t test_miauc 0.773 (0.019)\n",
            "\t train_miauc 1.000 (0.000)\n",
            "\t test_migmean 0.772 (0.019)\n",
            "\t train_migmean 1.000 (0.000)\n",
            ">> RF\n",
            "\t test_miaccuracy 0.825 (0.026)\n",
            "\t train_miaccuracy 1.000 (0.000)\n",
            "\t test_miprecision 0.778 (0.031)\n",
            "\t train_miprecision 1.000 (0.000)\n",
            "\t test_mirecall 0.910 (0.024)\n",
            "\t train_mirecall 1.000 (0.000)\n",
            "\t test_mifi 0.839 (0.022)\n",
            "\t train_mifi 1.000 (0.000)\n",
            "\t test_miauc 0.909 (0.022)\n",
            "\t train_miauc 1.000 (0.000)\n",
            "\t test_migmean 0.820 (0.028)\n",
            "\t train_migmean 1.000 (0.000)\n",
            ">> XGBoost\n",
            "\t test_miaccuracy 0.818 (0.023)\n",
            "\t train_miaccuracy 0.925 (0.006)\n",
            "\t test_miprecision 0.790 (0.035)\n",
            "\t train_miprecision 0.897 (0.008)\n",
            "\t test_mirecall 0.869 (0.022)\n",
            "\t train_mirecall 0.960 (0.007)\n",
            "\t test_mifi 0.827 (0.019)\n",
            "\t train_mifi 0.927 (0.006)\n",
            "\t test_miauc 0.906 (0.020)\n",
            "\t train_miauc 0.980 (0.003)\n",
            "\t test_migmean 0.816 (0.025)\n",
            "\t train_migmean 0.924 (0.006)\n",
            ">> MLP\n",
            "\t test_miaccuracy 0.812 (0.024)\n",
            "\t train_miaccuracy 0.833 (0.006)\n",
            "\t test_miprecision 0.798 (0.030)\n",
            "\t train_miprecision 0.817 (0.012)\n",
            "\t test_mirecall 0.839 (0.039)\n",
            "\t train_mirecall 0.858 (0.014)\n",
            "\t test_mifi 0.817 (0.024)\n",
            "\t train_mifi 0.837 (0.006)\n",
            "\t test_miauc 0.902 (0.022)\n",
            "\t train_miauc 0.919 (0.005)\n",
            "\t test_migmean 0.811 (0.024)\n",
            "\t train_migmean 0.833 (0.007)\n",
            ">> SVM\n",
            "\t test_miaccuracy 0.816 (0.021)\n",
            "\t train_miaccuracy 0.850 (0.005)\n",
            "\t test_miprecision 0.774 (0.025)\n",
            "\t train_miprecision 0.805 (0.006)\n",
            "\t test_mirecall 0.896 (0.026)\n",
            "\t train_mirecall 0.925 (0.012)\n",
            "\t test_mifi 0.830 (0.018)\n",
            "\t train_mifi 0.861 (0.005)\n",
            "\t test_miauc 0.902 (0.023)\n",
            "\t train_miauc 0.937 (0.003)\n",
            "\t test_migmean 0.812 (0.022)\n",
            "\t train_migmean 0.847 (0.005)\n"
          ]
        }
      ],
      "source": [
        "# Definimos a continuación la función que llamamos \"mis_modelos\" que incluye\n",
        "# todos los modelos que deseamos comparar en el ejercicio.\n",
        "\n",
        "\n",
        "def mis_modelos():\n",
        "  modelos, nombres = list(), list()\n",
        "\n",
        "\n",
        "  # ************* Incluye aquí tu código:**************************\n",
        "  #\n",
        "  # Deberás incluir en cada modelo los argumentos que consideres\n",
        "  # adecuados para que cada uno converja y no esté sobre-entrenado\n",
        "  # con respecto a la métrica de la exactitud (accuracy).\n",
        "\n",
        "\n",
        "  # Regresión Logística - Logistic Regression-LR:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "  # Este valor de \"penalty\" no se debe de cambiar, ya que define el modelo sin regularización.\n",
        "  modelos.append(LogisticRegression(penalty=None, solver='lbfgs', max_iter=2000, random_state=1))\n",
        "  nombres.append('LR')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # k-Vecinos más Cercanos : k-Nearest-Neighbors-kNN:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "\n",
        "  modelos.append(KNeighborsClassifier())\n",
        "  nombres.append('kNN')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Árbol de decisiones-DecisionTree-DT:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "  modelos.append(DecisionTreeClassifier(max_depth = None,       # None, 5, 3, 1\n",
        "                                        min_samples_split=2,   # 2,3,5, 20\n",
        "                                        #min_samples_leaf=1,  # trata al inicio de usar solo uno de estos, split o leaf, para su mejor comprensión.\n",
        "                                        #random_state=7\n",
        "                                        ))\n",
        "  nombres.append('DTree')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Bosque Aleatorio-RandomForest-RF:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "  modelos.append(RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=300))\n",
        "  nombres.append('RF')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # XGBoosting:\n",
        "  # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
        "  # https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "  modelos.append(XGBClassifier(booster='gbtree', learning_rate=0.3, max_depth=3, n_estimators=50, subsample=1.0, objective='binary:logistic', n_jobs=-1))\n",
        "  nombres.append('XGBoost')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Red neuronal de Perceptrón Multicapa-MLP:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "  modelos.append(MLPClassifier(activation='logistic', alpha=0.0001, hidden_layer_sizes=(50, 50), max_iter=100))\n",
        "  nombres.append('MLP')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Máquina de Vectores de Soporte-SVM:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "  modelos.append(SVC(kernel='rbf',\n",
        "                     C= 1.0,      # 1.0, 0.00001, 1000           # inversamente proporcional a la constante de regularización L2.\n",
        "                     gamma= 'scale',           # scale,  0.005\n",
        "                     #class_weight='balanced',     # Siempre puedes hacer uso del balanceo en caso de que ayude.\n",
        "                     #random_state=7\n",
        "                     ))\n",
        "  nombres.append('SVM')\n",
        "\n",
        "  return modelos, nombres\n",
        "\n",
        "\n",
        "# Técnica de sub muestreo (under sampling) y/o sobre muestreo (oversampling) utilizada:\n",
        "\n",
        "# Aplicando sub-muestreo con Near Miss:\n",
        "# near_miss = NearMiss(version=1, n_neighbors=3)\n",
        "# Xtv_uo, ytv_uo = near_miss.fit_resample(Xtraintest, ytraintest)\n",
        "\n",
        "# Aplicando sobre-muestreo con K-Means SMOTE:\n",
        "kmeans_smote = KMeansSMOTE(random_state=42)\n",
        "Xtv_uo, ytv_uo = kmeans_smote.fit_resample(Xtraintest, ytraintest)\n",
        "\n",
        "# Aplicando sobre-muestreo con SMOTE:\n",
        "# mi_uoSampling = SMOTE(random_state=42)\n",
        "# Xtv_uo, ytv_uo = mi_uoSampling.fit_resample(Xtraintest, ytraintest)\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *******************\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Entrenamos cada uno de los modelos y desplegamos la métricas de Train y Val.\n",
        "\n",
        "# NOTA: Observa que el método de Validación-Cruzada llama  a los resultados\n",
        "#       de \"validation\" como \"test\":\n",
        "\n",
        "modelos, nombres = mis_modelos()\n",
        "resultados = list()\n",
        "\n",
        "for i in range(len(modelos)):\n",
        "\n",
        "  # Definimos nuestro pipeline con las transformaciones y los modelos:\n",
        "  pipeline = Pipeline(steps=[('ct',columnasTransformer),('m',modelos[i])])\n",
        "\n",
        "  # Aplicaremos validación-cruzada:\n",
        "  micv = RepeatedStratifiedKFold(n_splits=5,\n",
        "                                 n_repeats=3,\n",
        "                                 random_state=5     # agreguemos una semilla para estabilizar resultados.\n",
        "                                 )\n",
        "\n",
        "\n",
        "  # Definimos las métricas que desamos recuperar:\n",
        "  mismetricas = {'miaccuracy':'accuracy','miprecision':'precision','mirecall':'recall',\n",
        "                 'mifi':'f1','miauc':'roc_auc','migmean':make_scorer(geometric_mean_score)}\n",
        "\n",
        "  # Llevamos a cabo el entrenamiento:\n",
        "  scores = cross_validate(pipeline,\n",
        "                          Xtv_uo,\n",
        "                          ytv_uo,\n",
        "                          scoring=mismetricas,\n",
        "                          cv=micv,\n",
        "                          return_train_score=True,\n",
        "                          )\n",
        "\n",
        "  # Guardemos el resultado de cada modelo para análisis posteriores.\n",
        "  resultados.append(scores)\n",
        "\n",
        "  # Desplegamos los valores de las métricas para verificar si no hay\n",
        "  # subentrenamiento o sobreentrenamiento:\n",
        "  print('>> %s' % nombres[i])\n",
        "  for j,k in enumerate(list(scores.keys())):\n",
        "    if j>1:\n",
        "      print('\\t %s %.3f (%.3f)' % (k, np.mean(scores[k]),np.std(scores[k])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores parámetros MLP: {'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'max_iter': 100}\n",
            "Mejor accuracy MLP: 0.7817488561260804\n",
            "\n",
            "\n",
            "Mejores parámetros RF: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mejor accuracy RF: 0.7960040671072699\n",
            "\n",
            "\n",
            "Mejores parámetros XGB: {'booster': 'gbtree', 'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 50, 'n_jobs': -1, 'objective': 'binary:logistic', 'subsample': 1.0}\n",
            "Mejor accuracy XGB: 0.8073945094051856\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creando una validación cruzada para los 3 mejores modelos\n",
        "\n",
        "############################ Red neuronal de Perceptrón Multicapa-MLP ############################\n",
        "mlp = MLPClassifier(random_state=1)\n",
        "\n",
        "# Parámetros a probar\n",
        "params_mlp = {\n",
        "    'hidden_layer_sizes': [(6,), (15, 15), (30,), (50, 50), (100,)], # número de neuronas en cada capa oculta.\n",
        "    'activation': ['logistic'],\n",
        "    'max_iter': [50, 100, 200, 500, 1000],\n",
        "    'alpha': [0.0001], # término de regularización L2.\n",
        "}\n",
        "\n",
        "# Validación cruzada\n",
        "grid_search_mlp = GridSearchCV(estimator=mlp, param_grid=params_mlp, cv=5, scoring='accuracy')\n",
        "\n",
        "# Entrenamiento\n",
        "grid_search_mlp.fit(Xtv_uo, ytv_uo)\n",
        "\n",
        "# Resultados en forma de lista\n",
        "print(f'Mejores parámetros MLP: {grid_search_mlp.best_params_}')\n",
        "print(f'Mejor accuracy MLP: {grid_search_mlp.best_score_}')\n",
        "print('\\n')\n",
        "\n",
        "############################ Bosque Aleatorio (RandomForest - RF) ############################\n",
        "rf = RandomForestClassifier(random_state=1)\n",
        "\n",
        "# Parámetros a probar\n",
        "params_rf = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [None, 1, 2, 3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 5, 15],\n",
        "}\n",
        "\n",
        "# Validación cruzada\n",
        "grid_search_rf = GridSearchCV(estimator=rf, param_grid=params_rf, cv=5, scoring='accuracy')\n",
        "\n",
        "# Entrenamiento\n",
        "grid_search_rf.fit(Xtv_uo, ytv_uo)\n",
        "\n",
        "# Resultados en forma de lista\n",
        "print(f'Mejores parámetros RF: {grid_search_rf.best_params_}')\n",
        "print(f'Mejor accuracy RF: {grid_search_rf.best_score_}')\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "############################ XGBoosting ############################\n",
        "xgb = XGBClassifier(random_state=1)\n",
        "\n",
        "# Parámetros a probar\n",
        "params_xgb = {\n",
        "    'booster': ['gbtree'],\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [1, 3, 6],\n",
        "    'learning_rate': [0.3, 0.01, 0.000001],\n",
        "    'subsample': [1.0, 0.9, 0.8, 0.5],\n",
        "    'objective': ['binary:logistic'],\n",
        "    'n_jobs': [-1]\n",
        "}\n",
        "\n",
        "# Validación cruzada\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=params_xgb, cv=5, scoring='accuracy')\n",
        "\n",
        "# Entrenamiento\n",
        "grid_search_xgb.fit(Xtv_uo, ytv_uo)\n",
        "\n",
        "# Resultados en forma de lista\n",
        "print(f'Mejores parámetros XGB: {grid_search_xgb.best_params_}')\n",
        "print(f'Mejor accuracy XGB: {grid_search_xgb.best_score_}')\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JkGo57zMXqn"
      },
      "source": [
        "## **Ejercicio 8**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n04HnK-ZX4vl"
      },
      "source": [
        "### **Escribe tus conclusiones finales de la actividad.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7s7iDgKYEn9"
      },
      "source": [
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "\n",
        "\n",
        "## **Conclusiones sobre los resultados obtenidos**\n",
        "\n",
        "Esta actividad nos ha ayudado a entender de una mejor manera, la forma en la que se prepara un modelo de machine learning y sobre todo los ajustes que se deben hacer para obtener el mejor rendimiento posible.\n",
        "\n",
        "Cabe mencionar que dentro de nuestros muchos ajustes, obtuvimos dos tipos de resultados:\n",
        "* A) Métricas altas, pero con sobre-entrenamiento, como el caso de XGBoost y Random Forest (RF)\n",
        "* B) Métricas aceptables (similares a las del reporte de IEEE), sin sobre-entrenamiento, como LogisticRegression (LR), Red neuronal de Perceptrón Multicapa (MLP), y Maquina de Vectores (SVM).\n",
        "\n",
        "Es importante mencionar que para el siguiente análisis se siguieron las instrucciones del PDF de esta actividad, asi como el [artículo IEEE](https://ieeexplore.ieee.org/document/9239944) para la selección de las técnicas de sub-muestreo y sobre-muestreo.\n",
        "\n",
        "## Optimización de Hiperparámetros\n",
        "La forma en la que obtuvimos los mejores hiperparámetros fue haciendo una comparativa exhaustiva mediante la ayuda de la función GridSearchCV sobre los 3 modelos que obtuvieron las métricas mas altas en la primer corrida del código.\n",
        "\n",
        "## Sub-muestreo y Sobre-muestreo\n",
        "Por tratarse de un caso con clases desbalanceadas era imperativo que hiciéramos uso de una o varias técnicas de sub-muestreo y/o sobre-muestreo. Dicho esto, probamos varias técnicas tanto en cadena como por separado, tales como: NearMiss, SMOTE, SMOTETomek y KMeansSMOTE. Finalmente la que mejor resultados arrojó fue la de **KMeansSMOTE** en solitario, que es precisamente la que mejores resultados dió en el artículo de IEEE.\n",
        "\n",
        "## Resultados finales\n",
        "A continuación presentamos los resultados obtenidos en el apartado de Exactitud (Accuracy), y mas importante aun la media geométrica (Gmean), pues es el valor que toma en cuenta el desbalanceo de clases.\n",
        "\n",
        "### MODELOS CON MAYOR PUNTUACIÓN\n",
        "\n",
        "**Random Forest (RF)**\n",
        "* test_miaccuracy 0.825 (0.026)\n",
        "* train_miaccuracy 1.000 (0.000)\n",
        "* test_migmean 0.820 (0.028)\n",
        "* train_migmean 1.000 (0.000)\n",
        "\n",
        "**XGBoost**\n",
        "* test_miaccuracy 0.818 (0.023)\n",
        "* train_miaccuracy 0.925 (0.006)\n",
        "* test_migmean 0.816 (0.025)\n",
        "* train_migmean 0.924 (0.006)\n",
        "\n",
        "Ambos modelos a pesar de tener las puntuaciones mas altas, sufren de sobre-entrenamiento, ya que la diferencia entre las pruebas y el entrenamiento es mayor a un 5%. Por lo que los descartamos como posibles opciones.\n",
        "\n",
        "\n",
        "### MODELOS CON MEJOR PUNTUACIÓN (sin sobre-entrenamiento)\n",
        "\n",
        "**Logistic Regression (LR)**\n",
        "* test_miaccuracy 0.815 (0.023)\n",
        "* train_miaccuracy 0.831 (0.004)\n",
        "* test_migmean 0.814 (0.023)\n",
        "* train_migmean 0.831 (0.004)\n",
        "\n",
        "**Maquina de Vectores (SVM)**\n",
        "* test_miaccuracy 0.816 (0.021)\n",
        "* train_miaccuracy 0.850 (0.005)\n",
        "* test_migmean 0.812 (0.022)\n",
        "* train_migmean 0.847 (0.005)\n",
        "\n",
        "**Red neuronal de Perceptrón Multicapa (MLP)**\n",
        "* test_miaccuracy 0.812 (0.024)\n",
        "* train_miaccuracy 0.833 (0.006)\n",
        "* test_migmean 0.811 (0.024)\n",
        "* train_migmean 0.833 (0.007)\n",
        "\n",
        "De estos 3 últimos modelos, el que tiene la mejor exactitud es SVM con 81.6%, pero la métrica que realmente considera el desbalanceo de clases es ela media geométrica y el modelo mejor puntuado es La Regresión Logística (LR) con 81.4%.\n",
        "\n",
        "\n",
        "De esta forma podemos calificar como el mejor modelo de nuestro análisis a **Regresión Logística (LR)**, con un rendimiento similar al del [articulo IEEE](https://ieeexplore.ieee.org/document/9239944), y aun mas importante sin sufrir de sobre-entrenamiento, pues la diferencia entre las pruebas y el entrenamiento es de tan solo el 1.3%.\n",
        "\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto. +++++++++++\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-tW_CdUYMdl"
      },
      "source": [
        ">> ### **Fin de la Actividad de las Semanas 5 y 6.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "a08Di3GjkL1Y",
        "LckYCS8SlnFo",
        "qQx2lbzTkEsQ",
        "ocmGcSC2j_2x",
        "zb5bE4WJj8Rw",
        "UJokj9Diyeu0",
        "hZxyRbHL0gNF",
        "4JkGo57zMXqn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
